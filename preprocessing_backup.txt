def preprocessing_step1(sentence):
    sentence = sentence.lower()
    cleanr = re.compile('<.*?>')
    sentence = re.sub(cleanr, ' ', sentence)  #Removing HTML tags
    sentence = re.sub(r'\S+@\S+', 'EmailId', sentence)  #hardcoding EmailId instead of individual email id
    sentence = re.sub(r'\'', '', sentence, re.I|re.A) #replacing quotes with space
    sentence = re.sub(r'[0-9]', '', sentence, re.I|re.A) #Removing Numbers
    sentence = re.sub(r'[^a-zA-Z0-9\s]', ' ', sentence) ##Removing punctuations
    sentence = sentence.lower()
    sentence = re.sub(r'com ', ' ', sentence, re.I|re.A)
    sentence = re.sub(r'hello ', ' ', sentence, re.I|re.A)
    final_output = preprocessing_step2(sentence) #function call for tokenization,postag,lemmatization
    return ' '.join(final_output)

def preprocessing_step2(sentence):
    tokenized=word_tokenize(sentence)    
    pos_tagged=nltk.pos_tag(tokenized)
    wordnet_tagged=map(lambda x: (x[0],get_wordnet_pos(x[1])),pos_tagged)
    lemmatized_sentence=[]
    for word, tag in wordnet_tagged:
        if tag is None:
            lemmatized_sentence.append(word)
        else:        
            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))
    lemmatized_output= " ".join(lemmatized_sentence)
    preprocessing_step2_output = stopword_removal(lemmatized_output) #functioncall for stopword removal
    return preprocessing_step2_output

def stopword_removal(lemmatized_sen):
    stopwords_set = set(stopwords.words('english'))
    stopwords_filtered = [word for word in stopwords_set if word not in ["not","no","nor","don't","aren't","couldn't", "didn't", "doesn't", "hadn't", "hasn't", "haven't","isn't","mightn't","mustn't",  "needn't","shouldn't","wasn't","weren't","won't","wouldn't"]]
    stopword_output=[]
    stopword_output=[word for word in lemmatized_sen.split() if word not in stopwords_filtered]
    return stopword_output